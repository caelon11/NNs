{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a846db0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# native python libraries imports \n",
    "import math\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# sklearn imports \n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# keras imports \n",
    "import tensorflow\n",
    "import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras_tuner.tuners import RandomSearch, BayesianOptimization, Sklearn\n",
    "from keras_tuner.engine.hyperparameters import HyperParameters\n",
    "from keras.activations import relu, sigmoid\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.utils import get_file\n",
    "\n",
    "# required for compatibility between sklearn and keras\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7dc8c0",
   "metadata": {},
   "source": [
    "**DETERMINE OPTIMAL NUM LAYERS, NUM NODES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3c6f53ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_quickdraw10(file = './quickdraw10.npz'):\n",
    "    '''\n",
    "    Retrieves the quickdraw dataset and separates into training and testing classes\n",
    "    \n",
    "    Parameters:\n",
    "    ------------\n",
    "    file: str: location of quickdraw file on local machine\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X_train: numpy array list[list[int]]]: pixel values for all images that the model will be trained on\n",
    "    X_test: numpy array list[list[int]]: pixel values for all images that the model will test predictions on\n",
    "    y_train: numpy array list[int]: list of answer values for what number the picture represents; used in training\n",
    "    y_test: numpy array list[int]: list of answer values for what the picture represents; used to determine model acc\n",
    "    '''\n",
    "    data = np.load(file)\n",
    "\n",
    "    X = data['arr_0']\n",
    "    Y = data['arr_1']\n",
    "        \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, shuffle=True)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def create_model(n_layers, first_layer_nodes, last_layer_nodes, act_funct = \"relu\", negative_node_incrementation=True):\n",
    "    \"\"\"\"\n",
    "    Returns a compiled keras model \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_layers: int: number of hidden layers in model (excludes input/output)\n",
    "    first_layer_nodes: int: number of nodes in the first hidden layer \n",
    "    last_layer_nodes: int: number of nodes in the last hidden layer\n",
    "    act_funct: str: name of activation function to use in hidden layers (excluding output layer)\n",
    "    negative_node_incrementation: bool: whether subsequent hidden layers have more/less nodes\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    model: keras object: compiled neural network model ready to be fit on a dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def gen_layer_nodes(n_layers, first_layer_nodes, last_layer_nodes, negative_node_incrementation=True):\n",
    "        \"\"\"\n",
    "        Generates and returns the number of nodes in each hidden layer.  \n",
    "\n",
    "        Note\n",
    "        ----\n",
    "        Number of nodes in each layer is linearly incremented. \n",
    "        e.g., gen_layer_nodes(5, 500, 100) will generate [500, 400, 300, 200, 100]\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_layers: int: number of hidden layers\n",
    "        first_layer_nodes: int: number of nodes in first hidden layer\n",
    "        last_layer_nodes: int: number of nodes in the last hidden layer\n",
    "        negative_node_incrementation: bool: whether subsequent hidden layers have more/less nodes\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        layers: list[int]: Contains number of nodes for each layer \n",
    "        \"\"\"\n",
    "\n",
    "        # throws an error if n_layers is less than 2 \n",
    "        assert n_layers >= 2, \"n_layers must be 2 or greater\"\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        # when set to True number of nodes are decreased for subsequent layers \n",
    "        if negative_node_incrementation:\n",
    "            # subtract this amount from previous layer's nodes in order to increment towards smaller numbers \n",
    "            nodes_increment = (last_layer_nodes - first_layer_nodes)/ (n_layers-1)\n",
    "            \n",
    "        # when set to False number of nodes are increased for subsequent layers\n",
    "        else:\n",
    "            # add this amount from previous layer's nodes in order to increment towards larger numbers \n",
    "            nodes_increment = (first_layer_nodes - last_layer_nodes)/ (n_layers-1)\n",
    "\n",
    "        nodes = first_layer_nodes\n",
    "\n",
    "        for i in range(1, n_layers + 1):\n",
    "            layers.append(math.ceil(nodes))\n",
    "\n",
    "            # increment nodes for next layer \n",
    "            nodes += nodes_increment\n",
    "\n",
    "        return layers\n",
    "    \n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    \n",
    "    n_nodes = gen_layer_nodes(n_layers, first_layer_nodes, last_layer_nodes, negative_node_incrementation)\n",
    "    \n",
    "    for i in range(1, n_layers):\n",
    "        if i == 1:\n",
    "            model.add(Dense(first_layer_nodes, input_dim = X_train.shape[1], activation = act_funct))\n",
    "        else:\n",
    "            model.add(Dense(n_nodes[i-1], activation = act_funct))\n",
    "            \n",
    "            \n",
    "    # output layer \n",
    "    model.add(Dense(10, # 10 unit/neurons in output layer because we have 10 possible labels to predict  \n",
    "                    activation='softmax')) # softmax used for a label set greater than 2            \n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', \n",
    "                  optimizer='adam', # adam is a good default optimizer \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # do not include model.fit() inside the create_model function\n",
    "    # KerasClassifier is expecting a compiled model \n",
    "    return model\n",
    "\n",
    "def simpler_model(hp):\n",
    "    '''\n",
    "    Returns a compiled keras model ready for keras-tuner gridsearch algorithms \n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    hp: keras object: our hyperparameters that we will be tuning\n",
    "    \n",
    "    Returns:\n",
    "    -----------\n",
    "    model: keras object: a keras compiled model ready for tuning and then fitting\n",
    "    '''\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    # hidden layer\n",
    "    model.add(Dense(units=hp.get('units'),activation=hp.get(\"activation\")))\n",
    "    \n",
    "    # output layer\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(hp.get('learning_rate')),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def sklearn_model(units, learning_rate, activation):    \n",
    "    '''\n",
    "    Returns a compiled keras model ready for keras-tuner gridsearch algorithms \n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    units: list[int]: list of possible nodes to to evaluate in the models hidden layer\n",
    "    learning_rate: list[int]: list of possible learning rates to evaluate in the model\n",
    "    activations: list[str]: list of possible activation functions to evaluate in the model\n",
    "    \n",
    "    Returns:\n",
    "    -----------\n",
    "    model: keras object: compiled keras model ready for tuning and fitting to data\n",
    "    '''\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    # hidden layer\n",
    "    model.add(Dense(units, activation=activation))\n",
    "    \n",
    "    # output layer\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e515dbe0",
   "metadata": {},
   "source": [
    "**AUTOMATING NODE COUNT BETWEEN FIRST AND FINAL HIDDEN LAYERS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "beb1250a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 500)               392500    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 456)               228456    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 412)               188284    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 367)               151571    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 323)               118864    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 278)               90072     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 234)               65286     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 189)               44415     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 145)               27550     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                1460      \n",
      "=================================================================\n",
      "Total params: 1,308,458\n",
      "Trainable params: 1,308,458\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# LOADING DATASET\n",
    "X_train, X_test, y_train, y_test = load_quickdraw10()\n",
    "\n",
    "# Create model where first layers nodes are 500 and continuously decrease until the final layer\n",
    "model = create_model(n_layers = 10, first_layer_nodes = 500, last_layer_nodes = 100, act_funct = 'relu')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca9cc891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 500)               392500    \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 545)               273045    \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 589)               321594    \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 634)               374060    \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 678)               430530    \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 723)               490917    \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 767)               555308    \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 812)               623616    \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 856)               695928    \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 10)                8570      \n",
      "=================================================================\n",
      "Total params: 4,166,068\n",
      "Trainable params: 4,166,068\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create model where first layers nodes are 500 and continuously increase until the final layer\n",
    "model2 = create_model(n_layers = 10, first_layer_nodes = 500, last_layer_nodes = 100, act_funct = 'relu', \n",
    "                      negative_node_incrementation=False)\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10979175",
   "metadata": {},
   "source": [
    "**TUNING NODE COUNT + ACTIVATIONS + LEARNING RATE**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0c26a4",
   "metadata": {},
   "source": [
    "**Picking Hyperparameters and Number of Combinations to Sample**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d14c4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick hyperparameters to tune and the range of values for each to evaluate\n",
    "hp = HyperParameters()\n",
    "hp.Int('units', min_value = 32, max_value = 512, step = 32)\n",
    "hp.Choice('learning_rate', values = [1e-1, 1e-2, 1e-3])\n",
    "hp.Choice('activation', values = ['relu', 'sigmoid'])\n",
    "\n",
    "# Determine the number of unique hyperparameter combinations that can be evaluated\n",
    "# Nodes = 512 / 32 | Learning Rate = 3 | Activation Choices = 2\n",
    "num_unique_hp_combs = (512 / 32) * 3 * 2\n",
    "\n",
    "# Determine the number of combinations to actually sample\n",
    "hp_combs_sampled = num_unique_hp_combs * 0.33"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1692b01",
   "metadata": {},
   "source": [
    "**OPTIMIZED SEARCH METHOD 1: RANDOM SEARCH**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "311eef6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 32 Complete [00h 00m 19s]\n",
      "val_accuracy: 0.44683998823165894\n",
      "\n",
      "Best val_accuracy So Far: 0.7750800251960754\n",
      "Total elapsed time: 00h 05m 56s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "# Set up the random search algorithm\n",
    "random_tuner = RandomSearch(\n",
    "                            simpler_model,\n",
    "                            objective = 'val_accuracy', # define the most optimal combination\n",
    "                            max_trials = hp_combs_sampled, # num times to sample parameters & build model \n",
    "                            seed = 1234,\n",
    "                            hyperparameters = hp, # pass in the hyperparameter dictionary\n",
    "                            directory = './keras-tuner-trial',\n",
    "                            project_name = 'random_search')\n",
    "\n",
    "# Run the random search algorithm\n",
    "random_tuner.search(X_train,\n",
    "                    y_train,\n",
    "                    epochs = 3,\n",
    "                    validation_data = (X_test, y_test))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e5ec5015",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDOM SEARCH RESULTS\n",
      "Best Val Score: 0.7751\n",
      "Nodes: 352\n",
      "Learning Rate: 0.001\n",
      "Activation: Relu\n"
     ]
    }
   ],
   "source": [
    "# Identify the best score and hyperparameter combination for Random search approach\n",
    "# random_tuner.results_summary()\n",
    "print('RANDOM SEARCH RESULTS')\n",
    "print('Best Val Score: 0.7751')\n",
    "print('Nodes: 352')\n",
    "print('Learning Rate: 0.001')\n",
    "print('Activation: Relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba338db9",
   "metadata": {},
   "source": [
    "**OPTIMIZED SEARCH METHOD 2: BAYESIAN SEARCH**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec2e1459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 16 Complete [00h 00m 15s]\n",
      "val_accuracy: 0.7839199900627136\n",
      "\n",
      "Best val_accuracy So Far: 0.7839199900627136\n",
      "Total elapsed time: 00h 03m 39s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "# Bayesian approach runs trials with hps informed by prior trial runs and those prior hps combinations\n",
    "# Should be able to run fewer trials and get same or better results than random search\n",
    "max_trials = hp_combs_sampled * .5\n",
    "\n",
    "# Number of trials to run before being used as an influence in future trail combinations\n",
    "num_initial_points = 5\n",
    "\n",
    "# Variance-threshold for the hyperparameters; \n",
    "# i.e., higher is looking to find global min, lower more likely to converge on local minimum\n",
    "beta = 2.5\n",
    "\n",
    "# Set up the Bayesian search\n",
    "bayesian_tuner = BayesianOptimization(\n",
    "                    simpler_model,\n",
    "                    objective = 'val_accuracy',\n",
    "                    max_trials = max_trials,\n",
    "                    hyperparameters = hp,\n",
    "                    num_initial_points = num_initial_points, \n",
    "                    beta = beta, \n",
    "                    seed = 1234,\n",
    "                    directory = './keras-tuner-trial',\n",
    "                    project_name = 'bayesian_optimization_4')\n",
    "\n",
    "# Run the baysian search \n",
    "bayesian_tuner.search(X_train,\n",
    "                      y_train,\n",
    "                      epochs = 3,\n",
    "                      validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bb1c57b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAYESIAN SEARCH RESULTS: 3.66 minutes\n",
      "Best Val Score: 0.7839\n",
      "Nodes: 416\n",
      "Learning Rate: 0.001\n",
      "Activation: Relu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Identify the best score and hyperparameter combination for Bayesian approach\n",
    "# bayesian_tuner.results_summary()\n",
    "print('BAYESIAN SEARCH RESULTS: 3.66 minutes')\n",
    "print('Best Val Score: 0.7839')\n",
    "print('Nodes: 416')\n",
    "print('Learning Rate: 0.001')\n",
    "print('Activation: Relu')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0f3aa4",
   "metadata": {},
   "source": [
    "**OPTIMIZED SEARCH METHOD 3: BRUTE FORCE SEARCH**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1410032a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 96 candidates, totalling 288 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-20 14:10:10.785316: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-10-20 14:10:10.796779: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-10-20 14:10:10.821594: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-10-20 14:10:10.827326: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-10-20 14:10:10.827640: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-10-20 14:10:10.829429: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-10-20 14:10:10.845224: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-10-20 14:10:10.894767: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-10-20 14:10:10.899668: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-10-20 14:10:10.902605: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-10-20 14:10:10.906213: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-10-20 14:10:10.918528: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-10-20 14:10:10.918812: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-10-20 14:10:10.921602: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 4s 3ms/step - loss: 28.5865 - accuracy: 0.1052\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 57.0685 - accuracy: 0.1014\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 25.2941 - accuracy: 0.1019\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 53.2158 - accuracy: 0.1063\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 76.4496 - accuracy: 0.1073\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 52.7592 - accuracy: 0.1176\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 149.5735 - accuracy: 0.1115\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 2.3227 - accuracy: 0.0970\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 2.5761 - accuracy: 0.1004\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 2.3896 - accuracy: 0.0993\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 2.4820 - accuracy: 0.1004\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 2.3854 - accuracy: 0.102588\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 2.4088 - accuracy: 0.10315\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 2.5090 - accuracy: 0.10041\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 95.1477 - accuracy: 0.1157\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 85.5157 - accuracy: 0.1087\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 143.4048 - accuracy: 0.1103\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 110.0530 - accuracy: 0.1085\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 130.0351 - accuracy: 0.1153\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 101.8716 - accuracy: 0.1270\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 2.4125 - accuracy: 0.0985\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 2.3532 - accuracy: 0.1006\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 3.8458 - accuracy: 0.0997\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 2.3779 - accuracy: 0.1041\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 2.4967 - accuracy: 0.10083\n",
      "1563/1563 [==============================] - 7s 3ms/step - loss: 209.3782 - accuracy: 0.1137\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.4548 - accuracy: 0.1005\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 2.5700 - accuracy: 0.0972\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 183.5914 - accuracy: 0.1134\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 194.6406 - accuracy: 0.1133\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 119.3095 - accuracy: 0.1531\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 150.7584 - accuracy: 0.1086\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.4169 - accuracy: 0.0992\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.6659 - accuracy: 0.1012\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.6638 - accuracy: 0.1028\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 3.2955 - accuracy: 0.1009\n",
      "1563/1563 [==============================] - 8s 4ms/step - loss: 155.5122 - accuracy: 0.1327\n",
      "1563/1563 [==============================] - 8s 4ms/step - loss: 217.6773 - accuracy: 0.1218\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 3.0921 - accuracy: 0.1010\n",
      "1563/1563 [==============================] - 8s 4ms/step - loss: 151.7367 - accuracy: 0.1320\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 2.6546 - accuracy: 0.1037\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 2.4513 - accuracy: 0.1015\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 144.3597 - accuracy: 0.1357\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 237.3028 - accuracy: 0.1206\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 213.5462 - accuracy: 0.1360\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 190.7100 - accuracy: 0.1383\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 2.9220 - accuracy: 0.1008\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 2.4809 - accuracy: 0.1010\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 2.5698 - accuracy: 0.0976\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 2.6254 - accuracy: 0.1005\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 209.3800 - accuracy: 0.1139\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 201.7795 - accuracy: 0.1443\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 2.8689 - accuracy: 0.1042\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 440.0011 - accuracy: 0.1278\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 2.6989 - accuracy: 0.0987\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 171.7179 - accuracy: 0.1254\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 191.2432 - accuracy: 0.1207\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 269.6073 - accuracy: 0.1368\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 3.1570 - accuracy: 0.0994\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 211.6482 - accuracy: 0.1362\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 3.4259 - accuracy: 0.1012\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 2.5636 - accuracy: 0.1039\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 2.5651 - accuracy: 0.1008\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 2.6888 - accuracy: 0.0996\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 198.7978 - accuracy: 0.1298\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 211.0464 - accuracy: 0.1387\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 2.8727 - accuracy: 0.1048\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 2.7543 - accuracy: 0.0993\n",
      "1563/1563 [==============================] - 12s 7ms/step - loss: 355.7501 - accuracy: 0.1304\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 231.0182 - accuracy: 0.1508\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 243.0019 - accuracy: 0.1360\n",
      "1563/1563 [==============================] - 12s 7ms/step - loss: 296.5502 - accuracy: 0.1494\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 236.0044 - accuracy: 0.1242\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.5227 - accuracy: 0.097646.....................] - ETA: 2s - loss: 2.3226 - ac\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.7281 - accuracy: 0.1005\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 4.7077 - accuracy: 0.1045\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.6806 - accuracy: 0.1003\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 3.1242 - accuracy: 0.10102\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 651.4493 - accuracy: 0.1436\n",
      "1563/1563 [==============================] - 12s 7ms/step - loss: 286.3140 - accuracy: 0.1259\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 2.8409 - accuracy: 0.0979\n",
      "1563/1563 [==============================] - 12s 7ms/step - loss: 266.5999 - accuracy: 0.1298.........] - ETA: 6s - loss: 754.\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 272.2099 - accuracy: 0.1223\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 272.9674 - accuracy: 0.1489\n",
      "782/782 [==============================] - 5s 7ms/step - loss: 3.7133 - accuracy: 0.0976\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 685.7197 - accuracy: 0.1176\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 415.0399 - accuracy: 0.1522\n",
      "782/782 [==============================] - 5s 5ms/step - loss: 3.1196 - accuracy: 0.1002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 5s 6ms/step - loss: 3.3915 - accuracy: 0.1069\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 5.8433 - accuracy: 0.1004\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.7791 - accuracy: 0.1004\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 3.7230 - accuracy: 0.1110\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 340.2609 - accuracy: 0.1463\n",
      "1563/1563 [==============================] - 5s 2ms/step - loss: 6.2566 - accuracy: 0.1507\n",
      "1563/1563 [==============================] - 4s 2ms/step - loss: 4.7225 - accuracy: 0.1964\n",
      "1563/1563 [==============================] - 4s 2ms/step - loss: 5.8403 - accuracy: 0.1310\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 3.5780 - accuracy: 0.0971\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 434.0630 - accuracy: 0.1323\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 2.3084 - accuracy: 0.0994\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 2.2982 - accuracy: 0.1027\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 2.2025 - accuracy: 0.1392\n",
      "1563/1563 [==============================] - 5s 2ms/step - loss: 10.8714 - accuracy: 0.2344\n",
      "1563/1563 [==============================] - 5s 2ms/step - loss: 7.5454 - accuracy: 0.2576\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 2.0118 - accuracy: 0.2238\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 2.2217 - accuracy: 0.1386\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 3.1595 - accuracy: 0.1022\n",
      "1563/1563 [==============================] - 4s 2ms/step - loss: 7.4124 - accuracy: 0.2128\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 16.4080 - accuracy: 0.3020\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 14.9549 - accuracy: 0.2768\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 10.1484 - accuracy: 0.2912\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 2.1760 - accuracy: 0.1730\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 2.0927 - accuracy: 0.2987\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 2.0514 - accuracy: 0.2211\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 2.2171 - accuracy: 0.2825\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 16.3432 - accuracy: 0.3047\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 11.4445 - accuracy: 0.3184\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 9.8083 - accuracy: 0.3121\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.0940 - accuracy: 0.2019\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 1.9152 - accuracy: 0.2862\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 2.1027 - accuracy: 0.2195\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 14.8135 - accuracy: 0.3083\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 16.1645 - accuracy: 0.3336\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 13.6164 - accuracy: 0.2652\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 1.9988 - accuracy: 0.2390\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 15.3010 - accuracy: 0.3101\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.0159 - accuracy: 0.2876\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.1546 - accuracy: 0.2134\n",
      "782/782 [==============================] - 3s 2ms/step - loss: 2.2270 - accuracy: 0.14720 \n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 16.3996 - accuracy: 0.3648\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 15.9227 - accuracy: 0.2943\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 15.9011 - accuracy: 0.3188\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 23.8743 - accuracy: 0.3437\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 1.9729 - accuracy: 0.2506\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 2.1014 - accuracy: 0.2153\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.0018 - accuracy: 0.2564\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 18.1957 - accuracy: 0.3765\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 1.9596 - accuracy: 0.2559\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 18.1036 - accuracy: 0.3188\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 23.4996 - accuracy: 0.3310\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.8038 - accuracy: 0.3402\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 2.3150 - accuracy: 0.1951\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 13.7066 - accuracy: 0.3520\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 1.9252 - accuracy: 0.3218\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 31.0303 - accuracy: 0.3602\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 23.4270 - accuracy: 0.3709\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 22.8413 - accuracy: 0.3637\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 2.0218 - accuracy: 0.2168\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.9965 - accuracy: 0.2494\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 1.9938 - accuracy: 0.3133\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.9834 - accuracy: 0.3010\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 22.9540 - accuracy: 0.3797\n",
      "1563/1563 [==============================] - 10s 5ms/step - loss: 22.6877 - accuracy: 0.3370\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 18.8335 - accuracy: 0.3709\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 1.9664 - accuracy: 0.2853\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 2.0287 - accuracy: 0.3320\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 2.0133 - accuracy: 0.3500\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 24.1283 - accuracy: 0.3714\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 19.9662 - accuracy: 0.3391\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 19.6764 - accuracy: 0.4153\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 2.0916 - accuracy: 0.2090\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 2.0385 - accuracy: 0.2714\n",
      "1563/1563 [==============================] - 11s 6ms/step - loss: 25.7339 - accuracy: 0.3863\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 2.0757 - accuracy: 0.2591\n",
      "782/782 [==============================] - 5s 5ms/step - loss: 1.9513 - accuracy: 0.2754\n",
      "1563/1563 [==============================] - 12s 6ms/step - loss: 31.0145 - accuracy: 0.4111\n",
      "1563/1563 [==============================] - 11s 6ms/step - loss: 25.0125 - accuracy: 0.3711\n",
      "1563/1563 [==============================] - 11s 6ms/step - loss: 20.9080 - accuracy: 0.3945\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.7307 - accuracy: 0.3603\n",
      "782/782 [==============================] - 5s 5ms/step - loss: 1.9347 - accuracy: 0.3338\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 31.4454 - accuracy: 0.3915\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 28.3980 - accuracy: 0.3711\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 25.7324 - accuracy: 0.3972\n",
      "782/782 [==============================] - 5s 5ms/step - loss: 1.9716 - accuracy: 0.3387\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.9131 - accuracy: 0.2794\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.8651 - accuracy: 0.3156\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.1320 - accuracy: 0.3631\n",
      "1563/1563 [==============================] - 11s 6ms/step - loss: 32.7964 - accuracy: 0.3888\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 28.0067 - accuracy: 0.4135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 5s 6ms/step - loss: 2.0070 - accuracy: 0.3237\n",
      "1563/1563 [==============================] - 12s 7ms/step - loss: 26.9686 - accuracy: 0.4011\n",
      "1563/1563 [==============================] - 12s 7ms/step - loss: 38.4323 - accuracy: 0.3805\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 1.9710 - accuracy: 0.2881\n",
      "1563/1563 [==============================] - 12s 7ms/step - loss: 34.0744 - accuracy: 0.3610\n",
      "1563/1563 [==============================] - 12s 7ms/step - loss: 24.9489 - accuracy: 0.4141\n",
      "1563/1563 [==============================] - 12s 7ms/step - loss: 19.4427 - accuracy: 0.4204\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 1.9645 - accuracy: 0.2856\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.9268 - accuracy: 0.2974\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.9859 - accuracy: 0.3199\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.9334 - accuracy: 0.2965  \n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 8.3564 - accuracy: 0.2817\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 1.9751 - accuracy: 0.2993\n",
      "1563/1563 [==============================] - 4s 2ms/step - loss: 7.2275 - accuracy: 0.2474\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 23.8681 - accuracy: 0.4174\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 1.7592 - accuracy: 0.3845\n",
      "1563/1563 [==============================] - 4s 2ms/step - loss: 8.9840 - accuracy: 0.2127\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 1.7574 - accuracy: 0.3647\n",
      "1563/1563 [==============================] - 4s 2ms/step - loss: 11.2902 - accuracy: 0.3922\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.9814 - accuracy: 0.2318\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 1.8933 - accuracy: 0.3262\n",
      "1563/1563 [==============================] - 5s 2ms/step - loss: 8.7649 - accuracy: 0.3960\n",
      "1563/1563 [==============================] - 5s 2ms/step - loss: 9.7200 - accuracy: 0.3386\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 10.4493 - accuracy: 0.4025\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 1.5213 - accuracy: 0.5063\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 1.5365 - accuracy: 0.5238\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 1.6059 - accuracy: 0.4336\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 1.4721 - accuracy: 0.5054\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 9.7012 - accuracy: 0.4621\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 10.3285 - accuracy: 0.4068\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 11.8306 - accuracy: 0.4976\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 1.3311 - accuracy: 0.5712\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 1.3871 - accuracy: 0.5634\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 11.3080 - accuracy: 0.4942\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 11.4712 - accuracy: 0.4560\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 1.3567 - accuracy: 0.5880\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 1.3217 - accuracy: 0.6339  \n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 12.1389 - accuracy: 0.5050\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 1.3769 - accuracy: 0.5883\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 14.7110 - accuracy: 0.5192\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 1.3195 - accuracy: 0.5985  \n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 12.0681 - accuracy: 0.5247\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 1.3605 - accuracy: 0.5779\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 12.9354 - accuracy: 0.5308\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 12.1702 - accuracy: 0.5366\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 1.2344 - accuracy: 0.6383\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 1.2058 - accuracy: 0.6434\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 11.8495 - accuracy: 0.5267\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 1.2393 - accuracy: 0.6064\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 11.9583 - accuracy: 0.5635\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 13.8383 - accuracy: 0.5637\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 1.1571 - accuracy: 0.6597\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 12.9057 - accuracy: 0.5560\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 1.1336 - accuracy: 0.6314\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 1.1854 - accuracy: 0.6654\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 1.1134 - accuracy: 0.6656\n",
      "1563/1563 [==============================] - 8s 4ms/step - loss: 13.4553 - accuracy: 0.5777\n",
      "1563/1563 [==============================] - 8s 4ms/step - loss: 13.7037 - accuracy: 0.6011\n",
      "1563/1563 [==============================] - 8s 4ms/step - loss: 13.6399 - accuracy: 0.5729\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.0899 - accuracy: 0.7023\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.1236 - accuracy: 0.7003\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 13.2783 - accuracy: 0.6042\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 13.0212 - accuracy: 0.5855\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 1.1521 - accuracy: 0.6659\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 13.0434 - accuracy: 0.5932\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 1.0150 - accuracy: 0.7132\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 1.0758 - accuracy: 0.7010\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.0634 - accuracy: 0.7031\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 13.4769 - accuracy: 0.5964 \n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 12.3005 - accuracy: 0.5921\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.1503 - accuracy: 0.6761\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 12.8753 - accuracy: 0.6002\n",
      "1563/1563 [==============================] - 10s 5ms/step - loss: 12.6008 - accuracy: 0.6032\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 1.0617 - accuracy: 0.6892\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 1.1169 - accuracy: 0.6984 \n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 13.6932 - accuracy: 0.6028\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 13.6522 - accuracy: 0.6116\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 13.6319 - accuracy: 0.6103\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.9969 - accuracy: 0.7195\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.1484 - accuracy: 0.6652\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.1000 - accuracy: 0.6897\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 1.0570 - accuracy: 0.6972\n",
      "1563/1563 [==============================] - 12s 6ms/step - loss: 14.0034 - accuracy: 0.6176\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 13.5861 - accuracy: 0.6069\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 13.7877 - accuracy: 0.5960\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 13.6905 - accuracy: 0.6303\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.0044 - accuracy: 0.7267\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 14.4787 - accuracy: 0.6139\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 13.8852 - accuracy: 0.6195\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.0125 - accuracy: 0.7188\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.0875 - accuracy: 0.7116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 4s 4ms/step - loss: 1.0224 - accuracy: 0.7218\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.0158 - accuracy: 0.7134\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 13.7823 - accuracy: 0.6348\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.0037 - accuracy: 0.71790\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 1.0464 - accuracy: 0.7093\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 13.6402 - accuracy: 0.6242\n",
      "1563/1563 [==============================] - 12s 7ms/step - loss: 14.7641 - accuracy: 0.6351\n",
      "1563/1563 [==============================] - 12s 7ms/step - loss: 13.8169 - accuracy: 0.6268\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 13.8382 - accuracy: 0.6309\n",
      "1563/1563 [==============================] - 12s 7ms/step - loss: 13.0612 - accuracy: 0.6369\n",
      "1563/1563 [==============================] - 12s 7ms/step - loss: 14.0481 - accuracy: 0.6270\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.0810 - accuracy: 0.7186\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.0045 - accuracy: 0.7348\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.0182 - accuracy: 0.7232\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.0281 - accuracy: 0.7139\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9554 - accuracy: 0.7358\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.0279 - accuracy: 0.7191\n",
      "1563/1563 [==============================] - 4s 2ms/step - loss: 2.1194 - accuracy: 0.2282\n",
      "1563/1563 [==============================] - 11s 6ms/step - loss: 14.5500 - accuracy: 0.6403\n",
      "1563/1563 [==============================] - 4s 2ms/step - loss: 1.9978 - accuracy: 0.2800\n",
      "1563/1563 [==============================] - 4s 2ms/step - loss: 2.1042 - accuracy: 0.2410\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 2.0010 - accuracy: 0.2266\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 2.0678 - accuracy: 0.2238\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 1.8395 - accuracy: 0.2734\n",
      "1563/1563 [==============================] - 4s 2ms/step - loss: 2.0780 - accuracy: 0.2918 \n",
      "1563/1563 [==============================] - 4s 2ms/step - loss: 2.0634 - accuracy: 0.2952\n",
      "1563/1563 [==============================] - 4s 2ms/step - loss: 2.1452 - accuracy: 0.2673\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.0012 - accuracy: 0.7279\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 1.9636 - accuracy: 0.3855\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 2.0795 - accuracy: 0.2553\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 1.8963 - accuracy: 0.3532\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 2.1826 - accuracy: 0.3138\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.0660 - accuracy: 0.3551\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.1353 - accuracy: 0.3180\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 2.1231 - accuracy: 0.3592\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 2.0571 - accuracy: 0.3553\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 2.3406 - accuracy: 0.3281\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 2.3688 - accuracy: 0.3545\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 2.2476 - accuracy: 0.3005\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 1.8509 - accuracy: 0.3786\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 2.4187 - accuracy: 0.3273\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 2.2610 - accuracy: 0.3628\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 3.1203 - accuracy: 0.2984\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 1.6474 - accuracy: 0.3986\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 2.4426 - accuracy: 0.2936\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 2.5073 - accuracy: 0.3515\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 2.5472 - accuracy: 0.3478\n",
      "1563/1563 [==============================] - 8s 4ms/step - loss: 2.4970 - accuracy: 0.3605\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 2.5738 - accuracy: 0.3859\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 2.5229 - accuracy: 0.4080\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 2.1538 - accuracy: 0.3413\n",
      "1563/1563 [==============================] - 8s 4ms/step - loss: 2.7316 - accuracy: 0.3830\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.0996 - accuracy: 0.3575\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 1.6309 - accuracy: 0.4732\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 1.8717 - accuracy: 0.5095\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 2.5737 - accuracy: 0.3189\n",
      "1563/1563 [==============================] - 8s 4ms/step - loss: 2.8619 - accuracy: 0.3766\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 3.1255 - accuracy: 0.3518\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 2.7152 - accuracy: 0.3540\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 2.9002 - accuracy: 0.3862\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 3.0174 - accuracy: 0.4015\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 3.2042 - accuracy: 0.4016\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 3.4826 - accuracy: 0.3689\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 3.0068 - accuracy: 0.4007\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 3.0719 - accuracy: 0.3809\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 3.4656 - accuracy: 0.3331\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 1.9659 - accuracy: 0.4638\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.0970 - accuracy: 0.4813\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 3.4800 - accuracy: 0.4044\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 3.3013 - accuracy: 0.3999\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 1.9065 - accuracy: 0.4423\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 3.3566 - accuracy: 0.3525\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 3.5416 - accuracy: 0.3633\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 3.2108 - accuracy: 0.3691\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 3.7207 - accuracy: 0.3973\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 3.9884 - accuracy: 0.3729\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 3.8707 - accuracy: 0.3893\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 3.9376 - accuracy: 0.3960\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 3.6979 - accuracy: 0.4060\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 2.7970 - accuracy: 0.3614\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 2.7952 - accuracy: 0.3866\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 3.5023 - accuracy: 0.3458\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 3.5928 - accuracy: 0.4056\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 4.1471 - accuracy: 0.3620\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 3.3105 - accuracy: 0.5111\n",
      "1563/1563 [==============================] - 11s 6ms/step - loss: 4.1753 - accuracy: 0.4015\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 3.9289 - accuracy: 0.4312\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 3.9169 - accuracy: 0.3840\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 3.9412 - accuracy: 0.4084\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 3.8290 - accuracy: 0.4093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 11s 7ms/step - loss: 4.1362 - accuracy: 0.3980\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 4.3516 - accuracy: 0.3788\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 4.2603 - accuracy: 0.4157\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 4.1301 - accuracy: 0.3892\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 4.4633 - accuracy: 0.3154\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 3.1283 - accuracy: 0.4705\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 2.9145 - accuracy: 0.4161\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 3.8604 - accuracy: 0.3954\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 3.4423 - accuracy: 0.4726\n",
      "1563/1563 [==============================] - 11s 6ms/step - loss: 4.4420 - accuracy: 0.4108\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 3.5131 - accuracy: 0.3987\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 4.3791 - accuracy: 0.3971\n",
      "1563/1563 [==============================] - 12s 7ms/step - loss: 4.8199 - accuracy: 0.3987\n",
      "1563/1563 [==============================] - 12s 7ms/step - loss: 5.4246 - accuracy: 0.3603\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 5.8917 - accuracy: 0.3839\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 4.7883 - accuracy: 0.3933\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 5.1077 - accuracy: 0.4058\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 3.2159 - accuracy: 0.46635 - accuracy: \n",
      "782/782 [==============================] - 5s 6ms/step - loss: 4.1960 - accuracy: 0.4468\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 3.0783 - accuracy: 0.4281\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 3.6923 - accuracy: 0.4449\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 3.8720 - accuracy: 0.3977\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 3.4610 - accuracy: 0.4680\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.6603 - accuracy: 0.4087\n",
      "1563/1563 [==============================] - 11s 6ms/step - loss: 5.6862 - accuracy: 0.3752\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.7413 - accuracy: 0.3826\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.7798 - accuracy: 0.3490\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 1.4310 - accuracy: 0.5102\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 1.5088 - accuracy: 0.4690\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 1.6402 - accuracy: 0.4135\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 1.4831 - accuracy: 0.4365\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 1.5782 - accuracy: 0.4535\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 1.5418 - accuracy: 0.4537\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 1.4779 - accuracy: 0.4665\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 1.3303 - accuracy: 0.5183\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 4.0414 - accuracy: 0.4613\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 1.3294 - accuracy: 0.5263\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.4600 - accuracy: 0.4918\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 1.5058 - accuracy: 0.4734\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 1.4836 - accuracy: 0.4827\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 1.2661 - accuracy: 0.5686\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.4858 - accuracy: 0.4871\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.4368 - accuracy: 0.5022\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.5345 - accuracy: 0.4585\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 1.4190 - accuracy: 0.5152\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 1.2885 - accuracy: 0.5352\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 1.4143 - accuracy: 0.5102\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 1.3139 - accuracy: 0.5565\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 1.2617 - accuracy: 0.5535\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 1.2642 - accuracy: 0.5548\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 1.2368 - accuracy: 0.5834\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.4730 - accuracy: 0.4789\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.4476 - accuracy: 0.5014\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.4262 - accuracy: 0.5068\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 1.2587 - accuracy: 0.5618\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.3754 - accuracy: 0.5307\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.3833 - accuracy: 0.5261\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 1.4206 - accuracy: 0.5120\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 1.2400 - accuracy: 0.5891\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 1.2339 - accuracy: 0.5816\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 1.2156 - accuracy: 0.5934\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 1.1665 - accuracy: 0.6210\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 1.1908 - accuracy: 0.5947\n",
      "1563/1563 [==============================] - 8s 4ms/step - loss: 1.4223 - accuracy: 0.5138\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 1.4369 - accuracy: 0.5036\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.2686 - accuracy: 0.5603\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 1.3728 - accuracy: 0.5260\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.3922 - accuracy: 0.5297\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.3956 - accuracy: 0.5235\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.1989 - accuracy: 0.5809\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.4299 - accuracy: 0.5086\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 1.3168 - accuracy: 0.5584\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 1.2529 - accuracy: 0.5564\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.4101 - accuracy: 0.5156\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 1.2869 - accuracy: 0.5582\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 1.2701 - accuracy: 0.5860\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 1.2063 - accuracy: 0.5858\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 1.4068 - accuracy: 0.5214\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 1.4067 - accuracy: 0.5168\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 1.3664 - accuracy: 0.5373\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.4217 - accuracy: 0.5084\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.3483 - accuracy: 0.5376\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 1.1413 - accuracy: 0.6190\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.2775 - accuracy: 0.5723\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.1918 - accuracy: 0.6031\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.3470 - accuracy: 0.5430\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.3994 - accuracy: 0.5249\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.3975 - accuracy: 0.5216\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.3047 - accuracy: 0.5927\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.1278 - accuracy: 0.6192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 3s 4ms/step - loss: 1.2852 - accuracy: 0.5746\n",
      "1563/1563 [==============================] - 11s 6ms/step - loss: 1.3981 - accuracy: 0.5225\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.3215 - accuracy: 0.5523\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.3778 - accuracy: 0.5310\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.3977 - accuracy: 0.5292\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 1.1827 - accuracy: 0.6056\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.3888 - accuracy: 0.5268\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.3864 - accuracy: 0.5270\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.3880 - accuracy: 0.5348\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 1.1198 - accuracy: 0.6333\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.1538 - accuracy: 0.6142\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.1722 - accuracy: 0.6149\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.2752 - accuracy: 0.5650\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.2709 - accuracy: 0.5822\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.1777 - accuracy: 0.6108\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.3341 - accuracy: 0.5476\n",
      "1563/1563 [==============================] - 12s 7ms/step - loss: 1.3977 - accuracy: 0.5300\n",
      "1563/1563 [==============================] - 12s 7ms/step - loss: 1.4446 - accuracy: 0.5164\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.3548 - accuracy: 0.5485\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 1.3199 - accuracy: 0.6055\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.3602 - accuracy: 0.5439\n",
      "1563/1563 [==============================] - 12s 7ms/step - loss: 1.3816 - accuracy: 0.5341\n",
      "1563/1563 [==============================] - 12s 7ms/step - loss: 1.3662 - accuracy: 0.5417\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.3289 - accuracy: 0.5988\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.3590 - accuracy: 0.5504\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.2967 - accuracy: 0.6038\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.1832 - accuracy: 0.6176\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.3086 - accuracy: 0.5761\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 1.2233 - accuracy: 0.6152\n",
      "1563/1563 [==============================] - 4s 2ms/step - loss: 1.6188 - accuracy: 0.4732\n",
      "1563/1563 [==============================] - 4s 2ms/step - loss: 1.6340 - accuracy: 0.4724\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 1.0777 - accuracy: 0.6675\n",
      "1563/1563 [==============================] - 4s 2ms/step - loss: 1.6053 - accuracy: 0.4834\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 1.1353 - accuracy: 0.6314\n",
      "1563/1563 [==============================] - 4s 2ms/step - loss: 1.4458 - accuracy: 0.5399\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 1.0837 - accuracy: 0.6674\n",
      "1563/1563 [==============================] - 4s 2ms/step - loss: 1.3917 - accuracy: 0.5527\n",
      "1563/1563 [==============================] - 4s 2ms/step - loss: 1.4502 - accuracy: 0.5360\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.3757 - accuracy: 0.5391\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 1.0373 - accuracy: 0.6752\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.9868 - accuracy: 0.6940\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.9823 - accuracy: 0.7029\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.3874 - accuracy: 0.5538\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 1.6675 - accuracy: 0.4590\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 1.3415 - accuracy: 0.5749\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.3818 - accuracy: 0.5552\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.9494 - accuracy: 0.6996\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.9588 - accuracy: 0.7089\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.9577 - accuracy: 0.7065\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.2730 - accuracy: 0.5941\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.3453 - accuracy: 0.5719\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.2783 - accuracy: 0.5938\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.9040 - accuracy: 0.7210\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.9164 - accuracy: 0.7168\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.9234 - accuracy: 0.7172\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 1.2473 - accuracy: 0.6052\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 1.2556 - accuracy: 0.6036\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.2784 - accuracy: 0.5989\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.8836 - accuracy: 0.7356\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 1.2287 - accuracy: 0.6178\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.9039 - accuracy: 0.7177\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.9309 - accuracy: 0.7132\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.2334 - accuracy: 0.6078\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.2277 - accuracy: 0.6155\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.8796 - accuracy: 0.7319\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.2208 - accuracy: 0.6180\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.9072 - accuracy: 0.7204\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.8826 - accuracy: 0.7229\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.8676 - accuracy: 0.7347\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 1.2030 - accuracy: 0.6246\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 1.2123 - accuracy: 0.6180\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9052 - accuracy: 0.7226\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1772 - accuracy: 0.6322\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 1.1872 - accuracy: 0.6242\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.8979 - accuracy: 0.7222\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.8859 - accuracy: 0.7260\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8630 - accuracy: 0.7346\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1774 - accuracy: 0.6320\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1663 - accuracy: 0.6319\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1591 - accuracy: 0.6363\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9011 - accuracy: 0.7240\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.8440 - accuracy: 0.7416\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.8971 - accuracy: 0.7272\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 1.1835 - accuracy: 0.6309\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 1.1597 - accuracy: 0.6309\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 1.1489 - accuracy: 0.6396\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.8616 - accuracy: 0.7434\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 1.1601 - accuracy: 0.6365\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8770 - accuracy: 0.7292\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8518 - accuracy: 0.7423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8491 - accuracy: 0.7442\n",
      "1563/1563 [==============================] - 10s 5ms/step - loss: 1.1356 - accuracy: 0.6435\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.1452 - accuracy: 0.6403\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.1305 - accuracy: 0.6472\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.8505 - accuracy: 0.7406\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.8399 - accuracy: 0.7401\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.8429 - accuracy: 0.7437\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.1257 - accuracy: 0.6505\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.1152 - accuracy: 0.6496\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.1194 - accuracy: 0.6468\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.1253 - accuracy: 0.6462\n",
      "782/782 [==============================] - 5s 5ms/step - loss: 0.8361 - accuracy: 0.7534\n",
      "782/782 [==============================] - 5s 5ms/step - loss: 0.8794 - accuracy: 0.7348\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.8295 - accuracy: 0.7506\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.8389 - accuracy: 0.7465\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.1079 - accuracy: 0.6529\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.1252 - accuracy: 0.6437\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.1070 - accuracy: 0.6512\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 0.8397 - accuracy: 0.7444\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.8152 - accuracy: 0.7480\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 0.8471 - accuracy: 0.7446\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.1109 - accuracy: 0.6513\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.0951 - accuracy: 0.6592\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.1006 - accuracy: 0.6520\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.0928 - accuracy: 0.6548\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 0.8603 - accuracy: 0.7292\n",
      "782/782 [==============================] - 5s 5ms/step - loss: 0.8465 - accuracy: 0.7381\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.8205 - accuracy: 0.7497\n",
      "782/782 [==============================] - 5s 5ms/step - loss: 0.8444 - accuracy: 0.7340\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.0942 - accuracy: 0.6580\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.0779 - accuracy: 0.6628\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.0685 - accuracy: 0.6682\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.8152 - accuracy: 0.7557\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.8146 - accuracy: 0.7533\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.8377 - accuracy: 0.7406\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.0855 - accuracy: 0.6581\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.8281 - accuracy: 0.7470\n",
      "2344/2344 [==============================] - 5s 2ms/step - loss: 1.0330 - accuracy: 0.6764\n"
     ]
    }
   ],
   "source": [
    "# build out the hyperparameter dictionary \n",
    "hyper_parameters = {\n",
    "                    \"units\": np.arange(32, 544, 32).tolist(),\n",
    "                    \"learning_rate\": [1e-1, 1e-2, 1e-3],\n",
    "                    \"activation\":[\"relu\", \"sigmoid\"]}\n",
    "\n",
    "# Create the model\n",
    "model = KerasClassifier(build_fn = sklearn_model)\n",
    "\n",
    "\n",
    "start = time() # Get start time of the brute force search\n",
    "\n",
    "\n",
    "# Create the Grid Search\n",
    "grid = GridSearchCV(estimator = model, \n",
    "                    param_grid = hyper_parameters, \n",
    "                    n_jobs = -2, \n",
    "                    verbose = 1, \n",
    "                    cv = 3)\n",
    "\n",
    "# Run the Grid Search\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "end = time() # Get the time at which the grid search finishes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dca73891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 1s 1ms/step - loss: 0.8094 - accuracy: 0.7517\n",
      "\n",
      "Brute Force Search Duration: 8.16 minutes\n",
      "Best Val Score: 0.7517\n",
      "Nodes: 512\n",
      "Learning Rate: 0.001\n",
      "Activation: sigmoid\n"
     ]
    }
   ],
   "source": [
    "# Get the best test accuracy\n",
    "best_model = grid_result.best_estimator_\n",
    "test_acc = best_model.score(X_test, y_test)\n",
    "print()\n",
    "# total run time \n",
    "total_run_time_in_minutes = (end - start) / 60\n",
    "\n",
    "# Report Results\n",
    "print(f'Brute Force Search Duration: {round(total_run_time_in_minutes, 2)} minutes')\n",
    "print('Best Val Score: 0.7517')\n",
    "print(f\"Nodes: {grid_result.best_params_['units']}\")\n",
    "print(f\"Learning Rate: {grid_result.best_params_['learning_rate']}\")\n",
    "print(f\"Activation: {grid_result.best_params_['activation']}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f3070c",
   "metadata": {},
   "source": [
    "**CONCLUSION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8cb7eecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BRUTE FORCE SEARCH RESULTS: 8.16 minutes\n",
      "Best Val Score: 0.7517\n",
      "Nodes: 512\n",
      "Learning Rate: 0.001\n",
      "Activation: sigmoid\n",
      "\n",
      "RANDOM SEARCH RESULTS: 5.93 minutes\n",
      "Best Val Score: 0.7751\n",
      "Nodes: 352\n",
      "Learning Rate: 0.001\n",
      "Activation: Relu\n",
      "\n",
      "BAYESIAN SEARCH RESULTS: 3.66 minutes\n",
      "Best Val Score: 0.7839\n",
      "Nodes: 416\n",
      "Learning Rate: 0.001\n",
      "Activation: Relu\n"
     ]
    }
   ],
   "source": [
    "print(f'BRUTE FORCE SEARCH RESULTS: {round(total_run_time_in_minutes, 2)} minutes')\n",
    "print('Best Val Score: 0.7517')\n",
    "print(f\"Nodes: {grid_result.best_params_['units']}\")\n",
    "print(f\"Learning Rate: {grid_result.best_params_['learning_rate']}\")\n",
    "print(f\"Activation: {grid_result.best_params_['activation']}\")\n",
    "print()\n",
    "\n",
    "print('RANDOM SEARCH RESULTS: 5.93 minutes')\n",
    "print('Best Val Score: 0.7751')\n",
    "print('Nodes: 352')\n",
    "print('Learning Rate: 0.001')\n",
    "print('Activation: Relu')\n",
    "print()\n",
    "\n",
    "print('BAYESIAN SEARCH RESULTS: 3.66 minutes')\n",
    "print('Best Val Score: 0.7839')\n",
    "print('Nodes: 416')\n",
    "print('Learning Rate: 0.001')\n",
    "print('Activation: Relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bcf71d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
