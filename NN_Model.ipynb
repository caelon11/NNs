{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19ff0850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# native python libraries imports \n",
    "import math\n",
    "from time import time\n",
    "import os\n",
    "from unittest import TestCase\n",
    "\n",
    "# Data Handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# sklearn imports \n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# keras imports \n",
    "import tensorflow\n",
    "import keras\n",
    "from keras import Sequential\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "from keras.layers import Flatten, Dense, Dropout, ReLU\n",
    "from keras.initializers import GlorotUniform\n",
    "from keras.activations import relu, sigmoid, tanh\n",
    "from keras.optimizers import Adam, SGD, Nadam\n",
    "from keras.utils import get_file\n",
    "from keras_tuner.tuners import RandomSearch, BayesianOptimization, Sklearn\n",
    "from keras_tuner.engine.hyperparameters import HyperParameters\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "# required for compatibility between sklearn and keras\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b0d02ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_quickdraw10(file = './quickdraw10.npz'):\n",
    "    '''\n",
    "    Retrieves the quickdraw dataset and separates into training and testing classes\n",
    "    \n",
    "    Parameters:\n",
    "    ------------\n",
    "    file: str: location of quickdraw file on local machine\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X_train: numpy array list[list[int]]]: pixel values for all images that the model will be trained on\n",
    "    X_test: numpy array list[list[int]]: pixel values for all images that the model will test predictions on\n",
    "    y_train: numpy array list[int]: list of answer values for what number the picture represents; used in training\n",
    "    y_test: numpy array list[int]: list of answer values for what the picture represents; used to determine model acc\n",
    "    '''\n",
    "    data = np.load(file)\n",
    "\n",
    "    X = data['arr_0']\n",
    "    Y = data['arr_1']\n",
    "        \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, shuffle=True)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = load_quickdraw10()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92f66ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(n_layers, first_layer_nodes, last_layer_nodes, act_funct = \"relu\", negative_node_incrementation=True):\n",
    "    \n",
    "    def gen_layer_nodes(n_layers, first_layer_nodes, last_layer_nodes, negative_node_incrementation=True):\n",
    "        assert n_layers >= 2, \"n_layers must be 2 or greater\"\n",
    "        layers = []\n",
    "\n",
    "        if negative_node_incrementation:\n",
    "            nodes_increment = (last_layer_nodes - first_layer_nodes)/ (n_layers-1)\n",
    "        else:\n",
    "            nodes_increment = (first_layer_nodes - last_layer_nodes)/ (n_layers-1)\n",
    "\n",
    "        nodes = first_layer_nodes\n",
    "\n",
    "        for i in range(1, n_layers + 1):\n",
    "            layers.append(math.ceil(nodes))\n",
    "            nodes += nodes_increment\n",
    "\n",
    "        return layers\n",
    "    \n",
    "    # Get List of number of nodes for each layer\n",
    "    n_nodes = gen_layer_nodes(n_layers, first_layer_nodes, last_layer_nodes, negative_node_incrementation)\n",
    "    \n",
    "    # Build Model\n",
    "    model = Sequential()\n",
    "    \n",
    "    for i in range(1, n_layers):                  # Hidden Layers\n",
    "        if i == 1:\n",
    "            model.add(Dense(first_layer_nodes, input_dim = X_train.shape[1], activation = act_funct))\n",
    "        else:\n",
    "            model.add(Dense(n_nodes[i-1], activation = act_funct))\n",
    "            \n",
    "    model.add(Dense(10, activation='softmax'))    # Output Layer\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', \n",
    "                  optimizer='adam', \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc27e57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff2ade8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simpler_model(hp):\n",
    "    # instantiate model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # hidden layer\n",
    "    model.add(Dense(units=hp.get('units'),activation=hp.get(\"activation\")))\n",
    "    \n",
    "    # output layer\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    # compile model\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(hp.get('learning_rate')),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb231f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def created_model(hp):\n",
    "    \n",
    "    def gen_layer_nodes(hp):     # Function to automate number of nodes per layer\n",
    "        \n",
    "        layers = []\n",
    "        first_layer_nodes = hp.get('units')    # Starting layer nodes: a hyperparameter\n",
    "        last_layer_nodes = 100\n",
    "        num_layers = hp.get('layers')          # Number of total layers: a hyperparameter\n",
    "        \n",
    "        nodes_increment = (last_layer_nodes - first_layer_nodes)/ (num_layers-1)\n",
    "        nodes = first_layer_nodes\n",
    "\n",
    "        for i in range(1, num_layers + 1):\n",
    "            layers.append(math.ceil(nodes))\n",
    "            nodes += nodes_increment\n",
    "\n",
    "        return layers               # List of node count for each layer\n",
    "     \n",
    "    n_nodes = gen_layer_nodes(hp)   # Get List of node count for each layer by calling function\n",
    "    num_layers = hp.get('layers')\n",
    "    \n",
    "    # Build Model\n",
    "    model = Sequential() \n",
    "    for i in range(1, num_layers):                  \n",
    "        if i == 1:                                # Input Layer\n",
    "            model.add(Dense(hp.get('units'), input_dim = X_train.shape[1], activation = hp.get('activation')))\n",
    "        else:                                     # Hidden Layers\n",
    "            model.add(Dense(n_nodes[i-1], activation = hp.get('activation')))\n",
    "            model.add(Dropout(hp.get('drop_prob')))      \n",
    "    model.add(Dense(10, activation='softmax'))    # Output Layer\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss = 'sparse_categorical_crossentropy', \n",
    "                  optimizer = keras.optimizers.Nadam(hp.get('learning_rate')), \n",
    "                  metrics = ['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea637cc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pick hyperparameters\n",
    "hp = HyperParameters()\n",
    "hp.Int('units', min_value = 250, max_value = 750, step = 100)                    # Node Count\n",
    "hp.Int('layers', min_value = 2, max_value = 6, step = 1)                        # Layer Count\n",
    "hp.Choice('learning_rate', values = [1e-1, 1e-2, 1e-3])                         # Learning Rate\n",
    "hp.Choice('activation', values = ['relu', 'sigmoid', 'tanh'])                   # Activation\n",
    "hp.Choice('drop_prob', values = [0.0, 0.2, 0.4])                                # Dropout\n",
    "\n",
    "stop = EarlyStopping(monitor = 'val_loss', min_delta = .001, patience = 2)      # Early Stopping: Epochs\n",
    "\n",
    "number_unique_parameter_combinations = (500 / 100) * (4 / 1) * 3 * 3 * 3\n",
    "max_trials = number_unique_parameter_combinations * .1\n",
    "max_trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "697a59e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 87 Complete [00h 00m 54s]\n",
      "val_accuracy: 0.8461999893188477\n",
      "\n",
      "Best val_accuracy So Far: 0.8586400151252747\n",
      "Total elapsed time: 01h 18m 37s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "max_trials = num_unique_hp_combs * .05\n",
    "num_initial_points = 10\n",
    "beta = 3.0\n",
    "\n",
    "# Bayesian search set up\n",
    "bayesian_tuner = BayesianOptimization(\n",
    "                                      created_model,\n",
    "                                      objective = 'val_accuracy',\n",
    "                                      max_trials = max_trials,\n",
    "                                      hyperparameters = hp,\n",
    "                                      num_initial_points = num_initial_points, \n",
    "                                      beta = beta, \n",
    "                                      seed = 1234,\n",
    "                                      directory = './keras-tuner-trial',\n",
    "                                      project_name = 'bayesian_optimization_5')\n",
    "\n",
    "# Run the baysian search \n",
    "\n",
    "bayesian_tuner.search(X_train,\n",
    "                      y_train,\n",
    "                      epochs = 10,\n",
    "                      validation_data = (X_test, y_test),\n",
    "                      callbacks = [stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dfbdd538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Validation Score: 0.8586\n",
      "\n",
      "Dynamic\n",
      "\n",
      "Units: 600\n",
      "Layers: 8\n",
      "Learning Rate: 0.001\n",
      "Activation: Relu\n",
      "Dropout Rate: 0.0%\n",
      "Static\n",
      "\n",
      "Optimizer: Adam\n",
      "Batch Size: 32\n"
     ]
    }
   ],
   "source": [
    "# bayesian_tuner.results_summary()\n",
    "\n",
    "print('Best Validation Score: 0.8586')\n",
    "print()\n",
    "print('Dynamic')\n",
    "print()\n",
    "print('Units: 600')\n",
    "print('Layers: 8')\n",
    "print('Learning Rate: 0.001')\n",
    "print('Activation: Relu')\n",
    "print('Dropout Rate: 0.0%')\n",
    "print('Static')\n",
    "print()\n",
    "print('Optimizer: Adam')\n",
    "print('Batch Size: 32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7eb9abf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 33 Complete [00h 02m 57s]\n",
      "val_accuracy: 0.8725600242614746\n",
      "\n",
      "Best val_accuracy So Far: 0.8763200044631958\n",
      "Total elapsed time: 01h 23m 58s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "num_initial_points = 8\n",
    "beta = 3.0\n",
    "\n",
    "# Bayesian search set up\n",
    "bayesian_tuner = BayesianOptimization(\n",
    "                                      created_model,\n",
    "                                      objective = 'val_accuracy',\n",
    "                                      max_trials = max_trials,\n",
    "                                      hyperparameters = hp,\n",
    "                                      num_initial_points = num_initial_points, \n",
    "                                      beta = beta, \n",
    "                                      seed = 1234,\n",
    "                                      directory = './keras-tuner-trial3',\n",
    "                                      project_name = 'bayesian_optimization_5')\n",
    "\n",
    "# Run the baysian search \n",
    "# batch_sizes = [64, 128, 256, 512]\n",
    "# for batch_size in batch_sizes:\n",
    "bayesian_tuner.search(X_train,\n",
    "                      y_train,\n",
    "                      epochs = 10,\n",
    "                      validation_data = (X_test, y_test),\n",
    "                      batch_size = 32,\n",
    "                      callbacks = [stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86f3c673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in ./keras-tuner-trial3/bayesian_optimization_5\n",
      "Showing 10 best trials\n",
      "Objective(name='val_accuracy', direction='max')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 750\n",
      "layers: 6\n",
      "learning_rate: 0.001\n",
      "activation: relu\n",
      "drop_prob: 0.0\n",
      "Score: 0.8763200044631958\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 675\n",
      "layers: 6\n",
      "learning_rate: 0.001\n",
      "activation: relu\n",
      "drop_prob: 0.0\n",
      "Score: 0.8729599714279175\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 675\n",
      "layers: 6\n",
      "learning_rate: 0.001\n",
      "activation: relu\n",
      "drop_prob: 0.0\n",
      "Score: 0.8727200031280518\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 300\n",
      "layers: 8\n",
      "learning_rate: 0.001\n",
      "activation: relu\n",
      "drop_prob: 0.0\n",
      "Score: 0.8725600242614746\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 525\n",
      "layers: 8\n",
      "learning_rate: 0.001\n",
      "activation: relu\n",
      "drop_prob: 0.0\n",
      "Score: 0.8725600242614746\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 675\n",
      "layers: 6\n",
      "learning_rate: 0.001\n",
      "activation: relu\n",
      "drop_prob: 0.0\n",
      "Score: 0.8725600242614746\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 675\n",
      "layers: 6\n",
      "learning_rate: 0.001\n",
      "activation: relu\n",
      "drop_prob: 0.0\n",
      "Score: 0.872160017490387\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 675\n",
      "layers: 6\n",
      "learning_rate: 0.001\n",
      "activation: relu\n",
      "drop_prob: 0.0\n",
      "Score: 0.8717600107192993\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 675\n",
      "layers: 6\n",
      "learning_rate: 0.001\n",
      "activation: relu\n",
      "drop_prob: 0.0\n",
      "Score: 0.8713600039482117\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 675\n",
      "layers: 6\n",
      "learning_rate: 0.001\n",
      "activation: relu\n",
      "drop_prob: 0.0\n",
      "Score: 0.870959997177124\n"
     ]
    }
   ],
   "source": [
    "bayesian_tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea23a7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
